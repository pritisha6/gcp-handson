{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oJsg1ScKZCH"
      },
      "source": [
        "## Apache Beam (Additional Output) Data Processing Notebook\n",
        "\n",
        "This notebook demonstrates a basic data processing pipeline using Apache Beam. It reads employee department data, extracts employee names, and categorizes them into different outputs based on length and a specific starting character.\n",
        "\n",
        "### Input Format:\n",
        "\n",
        "The notebook expects a text file named `dept_data.txt` to be uploaded to the `/content/` directory. Each line in this file should represent a record, with fields separated by commas. The second field (index 1) of each line is treated as the employee's `name`.\n",
        "\n",
        "**Example `dept_data.txt` content:**\n",
        "```\n",
        "ID,Name,Age,Department,HireDate\n",
        "1,Marco,30,Sales,01-01-2020\n",
        "2,Rebekah,25,HR,15-03-2021\n",
        "3,Kyle,35,Engineering,01-07-2019\n",
        "```\n",
        "\n",
        "### Output Format:\n",
        "\n",
        "The pipeline produces three separate text files in the `/content/` directory, each containing a list of names based on specific criteria. Apache Beam's `WriteToText` creates sharded output files, so the actual filenames will include a shard identifier (e.g., `-00000-of-00001`).\n",
        "\n",
        "1.  **`/content/short_name*-of-*`**: Contains names where the length is less than or equal to a defined `cut_off_len` (currently 4 characters).\n",
        "2.  **`/content/long_name*-of-*`**: Contains names where the length is greater than the defined `cut_off_len` (currently 4 characters).\n",
        "3.  **`/content/marker_b*-of-*`**: Contains names that start with a specific `marker` character (currently 'K')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "TKDvBqUc6D_p"
      },
      "outputs": [],
      "source": [
        "# Install the Apache Beam SDK with GCP support\n",
        "!pip install --quiet apache-beam[gcp]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "# Configure logging to display INFO level messages with a specific format\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,   # Set logging level to INFO\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        ")\n",
        "\n",
        "logging.info(\"Logging is enabled!\")"
      ],
      "metadata": {
        "id": "6EMl-FZkEAbz"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "# Prompt the user to upload files (e.g., dept_data.txt)\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "uTvB0u9T6Tw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "import logging\n",
        "\n",
        "# Create a Beam Pipeline object\n",
        "p1=beam.Pipeline()\n",
        "\n",
        "# Define a DoFn (Do Function) to process elements and produce multiple outputs\n",
        "class ProcessOutputs(beam.DoFn):\n",
        "  def process(self,element,cut_off_len,marker):\n",
        "    # Extract the name from the input line (assuming name is the second field)\n",
        "    name = element.split(',')[1]\n",
        "\n",
        "    # Categorize names based on length and yield to appropriate tagged outputs\n",
        "    if len(name)<=cut_off_len:\n",
        "      yield beam.pvalue.TaggedOutput('Short_Name',name)\n",
        "    else:\n",
        "      yield beam.pvalue.TaggedOutput('Long_Name',name)\n",
        "\n",
        "    # Categorize names that start with a specific marker character\n",
        "    if name.startswith(marker):\n",
        "      # Log the processing of names starting with the marker (for debugging/monitoring)\n",
        "      logging.info(\"Processing name starting with %s: %s\", marker, name)\n",
        "      yield beam.pvalue.TaggedOutput('marker_b',name) # Explicitly tag for marker_b output\n",
        "\n",
        "# Construct the Apache Beam pipeline\n",
        "result_pcoll = (\n",
        "    p1\n",
        "    | beam.io.ReadFromText('/content/dept_data.txt')  # Read input data from text file\n",
        "    | beam.ParDo(ProcessOutputs(), cut_off_len=4, marker='K') # Apply custom DoFn with parameters\n",
        "      .with_outputs('Long_Name','Short_Name','marker_b') # Specify all tagged outputs\n",
        ")\n",
        "\n",
        "# Get the individual PCollections for each tagged output\n",
        "short_name = result_pcoll.Short_Name\n",
        "long_name = result_pcoll.Long_Name\n",
        "marker_b = result_pcoll.marker_b\n",
        "\n",
        "# Write each PCollection to a separate text file\n",
        "short_name | 'Write short names' >> beam.io.WriteToText('/content/short_name')\n",
        "long_name | 'Write long names' >> beam.io.WriteToText('/content/long_name')\n",
        "marker_b | 'Write names with b' >> beam.io.WriteToText('/content/marker_b')\n",
        "\n",
        "# Run the Beam pipeline\n",
        "p1.run()\n",
        "\n",
        "# Display the first 5 lines of each output file to verify results\n",
        "print(\"\\n--- Short Names ---\")\n",
        "!{('head -n 5 /content/short_name*')}\n",
        "print(\"\\n--- Long Names ---\")\n",
        "!{('head -n 5 /content/long_name*')}\n",
        "print(\"\\n--- Names starting with 'K' ---\")\n",
        "!{('head -n 5 /content/marker_b*')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKzmKj3I6fGX",
        "outputId": "e9ccf776-8621-4f01-88e6-4d6438b59a66"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n",
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n",
            "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Short Names ---\n",
            "Itoe\n",
            "Kyle\n",
            "Kyle\n",
            "Olga\n",
            "Kirk\n",
            "\n",
            "--- Long Names ---\n",
            "Marco\n",
            "Rebekah\n",
            "Edouard\n",
            "Kumiko\n",
            "Gaston\n",
            "\n",
            "--- Names starting with 'K' ---\n",
            "Kyle\n",
            "Kyle\n",
            "Kumiko\n",
            "Kirk\n",
            "Kaori\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/dept_data.txt') as file:\n",
        "  name_list = list()\n",
        "  for line in file:\n",
        "    name_list.append(line.split(',')[1]) # Extract the name (second column)\n",
        "  for name in name_list:\n",
        "    if name.startswith('K'):\n",
        "      print(name) # Print names that start with 'K'\n",
        "  # print(name_list) # Uncomment to print the full list of names"
      ],
      "metadata": {
        "id": "k76_bzw3Chcu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}